{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa3fcebe",
   "metadata": {},
   "source": [
    "# Gradient Descent (GD)\n",
    "Minimization of any function\n",
    "\n",
    "1. Start with initial values of $w$ and $b$\n",
    "2. Choose learning rate $\\alpha$\n",
    "3. Calculate the cost function\n",
    "\n",
    "$\\frac{\\partial}{\\partial w} J(w,b) = \\frac{1}{m} \\sum_{i=1}^{m} [f_{w,b}(x^{(i)}) - y] x^{(i)}$\n",
    "\n",
    "$ \\frac{\\partial}{\\partial b} J(w,b)=  \\frac{1}{m} \\sum_{i=1}^{m} [f_{w,b}(x^{(i)}) - y]$\n",
    "\n",
    "4. **simulteneously** Update the values\n",
    "\n",
    "$ w_1 = w_0 - \\alpha \\frac{\\partial}{\\partial w} J(w,b) $\n",
    "\n",
    "$ b_1 = b_0 - \\alpha \\frac{\\partial}{\\partial b} J(w,b) $\n",
    "\n",
    "5. Stop when minimized\n",
    "\n",
    "\n",
    "### Choice of learning rate $\\alpha$\n",
    "- too small $\\rightarrow$ GD slow, long time to converge\n",
    "- too large $\\rightarrow$ GD will overshoot, may not converge, in fact may diverge\n",
    "- fixed is ok $\\rightarrow$ The derivative gets smaller close to minima therefore step size reduces\n",
    "\n",
    "\n",
    "#### Mean Squared Error is Convex function\n",
    "Therefore there is only one minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a49d8",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent\n",
    "Each step of the GD uses all of the training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8151b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
