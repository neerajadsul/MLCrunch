{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2610ab07",
   "metadata": {},
   "source": [
    "#### Autonomous Helicopter\n",
    "\n",
    "$ Input[\\text{Position of Helicopter}] \\longrightarrow Output[\\text{Movement of control sticks}] $\n",
    "\n",
    "$ \\text{[reward function]} $\n",
    "\n",
    "Currently popular applications:\n",
    "- Robot Control\n",
    "- Factory Optimization\n",
    "- Playing games (including video games)\n",
    "- Financial Trading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b788a67",
   "metadata": {},
   "source": [
    "### Key concepts of Reinforcement Learning\n",
    "\n",
    "- state $S$: current state of the agent e.g. position\n",
    "\n",
    "- action $a$: action taken to change the state\n",
    "\n",
    "- reward $R(S, a)$: reward function for the given state and action\n",
    "\n",
    "- new_state $S'$: new state after the action\n",
    "\n",
    "#### Concept of Return\n",
    "\n",
    "$Return = R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\gamma^3 R_4 + \\dots + \\gamma^{(N-1)} R_N$\n",
    "\n",
    "where $\\gamma$ is discount factor $0<\\gamma\\approx1.00$\n",
    "\n",
    "#### Policy (Controller) $\\Pi$\n",
    "\n",
    "Goal is to come up with a controller that $\\Pi(a)$\n",
    "\n",
    "#### Markov Decision Process (MDP)\n",
    "- future only depends on the current state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626a4432",
   "metadata": {},
   "source": [
    "## State-action value function (Q or Q* function)\n",
    "\n",
    "$Q(S,a)$ = reward for action $a$ once, starting at state $S$ and then **behave optimally after that**.\n",
    "\n",
    "The best possible return from state $S$ is the action $a$ that gives $max_a Q(S,a)$\n",
    "\n",
    "### Bellman Equation\n",
    "\n",
    "$ Q(S,a) = R(s) + \\gamma max Q(S',a') $\n",
    "\n",
    "### Stochastic Environment (random)\n",
    "\n",
    "$\\text{Expected Return} = Average[R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\gamma^3 R_4 + \\dots + \\gamma^{(N-1)} R_N]$\n",
    "\n",
    "where $\\gamma$ is discount factor $0<\\gamma\\approx1.00$\n",
    "\n",
    "#### Stochastic Bellman Equation\n",
    "$ Q(S,a) = R(s) + \\gamma Average[ max Q(S',a') ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aaae3a",
   "metadata": {},
   "source": [
    "## Continuous State Space\n",
    "\n",
    "Autonomous Truck, Lunar Lander\n",
    "\n",
    "$ S = [x, y, \\dot{x}, \\dot{y}, \\theta, \\dot\\theta, L, R] $\n",
    "\n",
    "$x,y$ = position of the vehicle\n",
    "\n",
    "$\\dot x,\\dot y$ = speed of the vehicle\n",
    "\n",
    "$\\theta, \\dot\\theta$ = orientation and angular velocity (rate of change of orientation)\n",
    "\n",
    "$L, R$ = binary value left/right leg/wheel is grounded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f6f039",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning\n",
    "### DQN Algorithm\n",
    "\n",
    "1. Initialize NN randomly as guess of $Q(S,a)$\n",
    "2. Repeat \n",
    "    - Take actions in the lunar lander, Get $(s, a, R(s), s')$\n",
    "    - Store 10,000 most recent $(s, a, R(s), s')$ tuples\n",
    "3. Train neural network\n",
    "    - Create a training set of 10,000 examples using \n",
    "    $x = (s,a)$ and $y = R(s) + \\gamma max Q(S',a') $\n",
    "    - Train such that $Q_{new} \\approx y$\n",
    "4. Set $Q = Q_{new}$ and repeat from step-2.\n",
    "\n",
    "### $\\epsilon$ Greedy Policy\n",
    "\n",
    "While choosing the action $a$\n",
    "1. **Greedy Phase** : With $\\epsilon=0.95$, pick the action that maximizes Q(s,a).\n",
    "2. **Exploration Phase**: With $\\epsilon=0.05$, pick an action a randomly.\n",
    "\n",
    "In practice, $\\epsilon$ is smaller at start and increases as the training progresses. Essentially, more exploration at start and more greedy towards end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d8531a",
   "metadata": {},
   "source": [
    "## Algorithm Refinement\n",
    "### Mini-batches\n",
    "- Use subset of training examples for each iteration of gradient descent.\n",
    "- Noisy but fast\n",
    "\n",
    "### Soft-update\n",
    "\n",
    "$ Q(s,a) = Q_{new}(s',a') $ --> always update $Q$ with new value.\n",
    "\n",
    "$ Q(s,a) = (0.05)Q_{new}(s',a') + (1-0.05) Q $ --> update $Q$ with part of new value and old value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d394a42",
   "metadata": {},
   "source": [
    "### State of Reinforcement Learning\n",
    "- Much easier to get to work in a simulation than a real robot or system.\n",
    "- Far fewer real applications than supervised and unsupervised learning.\n",
    "- Exciting research is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef4534",
   "metadata": {},
   "source": [
    "## Deep Q-Learning (DQN)\n",
    "### Target Network\n",
    "\n",
    "$ y = R + \\gamma max Q(s', a': w) $\n",
    "Where $w$ are weights of the network.\n",
    "\n",
    "We are trying to minimize the error by adjusting $w$,\n",
    "\n",
    "$Error = R + \\gamma max Q(s', a': w) - Q(s,a:w) = y - Q(s,a:w) $\n",
    "\n",
    "Since $y$ will be chaning each iteration, the error minimization will oscillilate and cause instabilities.\n",
    "\n",
    "Therefore we use a clone network $\\hat{Q}(s,a:w^-)$ as the target $\\hat{Q}$ network.\n",
    "\n",
    "We use $\\hat{Q}$ network to generate $y$ and update $w^-$ using $w$ with **soft update**.\n",
    "\n",
    "$w^-_{new} \\leftarrow \\tau w + (1-\\tau) w^-$\n",
    "\n",
    "where $\\tau << 1$\n",
    "\n",
    "This ensures that $y$ changes slowly and improves the stability of our learning algorithms.\n",
    "\n",
    "### Experience Replay\n",
    "States, actions, and rewards within the environment are sequential. The agent will be biased due to strong correction between them. To resolve the issue, we store agent's experience in a memory buffer. To do the learning we randomly sample mini-batches from the buffer. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
